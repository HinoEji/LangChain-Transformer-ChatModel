{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a6f8b12",
   "metadata": {},
   "source": [
    "# Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f516bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage,ToolMessage, ToolCall\n",
    "# from langchain_core.utils.utils import ensure_id\n",
    "from typing import Any, Mapping, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76d79f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_hf_message_to_lc_message(_dict: Mapping[str, Any]) -> BaseMessage:\n",
    "  \"\"\"\n",
    "  Convert message  in huggingface style to Langchain Message style\n",
    "  \"\"\"\n",
    "  role = _dict[\"role\"]\n",
    "  content = _dict[\"content\"]\n",
    "  message = None\n",
    "\n",
    "  if role == \"system\":\n",
    "    message = SystemMessage(content=content)\n",
    "  elif role == \"user\":\n",
    "    message = HumanMessage(content=content)\n",
    "  elif role == \"assistant\":\n",
    "    message = AIMessage(content=content)\n",
    "  elif role == \"developer\":\n",
    "    # such model like : GPT-OSS\n",
    "    message = SystemMessage(content=content)\n",
    "  else:\n",
    "    raise ValueError(f\"Invalid role: {role}\")\n",
    "\n",
    "  return message\n",
    "\n",
    "def convert_hf_message_to_lc_message(messages: List[Mapping[str, Any]]) -> List[BaseMessage]:\n",
    "  \"\"\"\n",
    "  Convert messages in huggingface style to Langchain Message style\n",
    "  \"\"\"\n",
    "  chat_messages = []\n",
    "\n",
    "  for message in messages:\n",
    "    _message = _convert_hf_message_to_lc_message(message)\n",
    "    chat_messages.append(_message)\n",
    "\n",
    "  return chat_messages\n",
    "\n",
    "def convert_lc_message_to_hf_messages(messages: List[BaseMessage]) -> List[BaseMessage]:\n",
    "  \"\"\"\n",
    "  Convert messages in Langchain style to huggingface style\n",
    "  \"\"\"\n",
    "  chat_messages = []\n",
    "\n",
    "  for message in messages:\n",
    "    _type = message.type\n",
    "\n",
    "    if _type == \"system\":\n",
    "      role = \"system\"\n",
    "      content = message.content\n",
    "      msg = {\"role\" : \"system\", \"content\" : content}\n",
    "      chat_messages.append(msg)\n",
    "    elif _type == \"human\":\n",
    "      role = \"user\"\n",
    "      content = message.content\n",
    "      msg = {\"role\" : \"user\", \"content\" : content}\n",
    "      chat_messages.append(msg)\n",
    "    elif _type == \"tool\":\n",
    "      role = \"assistant\"\n",
    "      tool_call_id = message.tool_call_id\n",
    "      status = message.status\n",
    "      content = f\"Respone of tool_call_id {tool_call_id} with status {status} and content : {message.content}\"\n",
    "      msg = {\"role\" : \"assistant\", \"content\" : content}\n",
    "      chat_messages.append(msg)\n",
    "    elif _type == \"ai\":\n",
    "      role = \"assistant\"\n",
    "\n",
    "      # check if there is a tool call\n",
    "      tool_calls = message.tool_calls\n",
    "      if tool_calls:\n",
    "        # this is a tool call\n",
    "        for tool_call in tool_calls:\n",
    "          name = tool_call[\"name\"]\n",
    "          args = tool_call[\"args\"]\n",
    "          id = tool_call[\"id\"]\n",
    "          content = f\"Tool Calling:\\n tool_call_id : {id}\\n name : {name}\\n args : {args}\"\n",
    "          msg = {\"role\" : \"assistant\", \"content\" : content}\n",
    "          chat_messages.append(msg)\n",
    "      else:\n",
    "        content = message.content\n",
    "        msg = {\"role\" : \"assistant\", \"content\" : content}\n",
    "        chat_messages.append(msg)\n",
    "    else:\n",
    "      raise ValueError(f\"Invalid message type: {_type}\")\n",
    "\n",
    "\n",
    "  return chat_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c638f91b",
   "metadata": {},
   "source": [
    "# Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea8e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tool_call(\n",
    "    name: str,\n",
    "    args: dict[str, Any],\n",
    "    *,\n",
    "    id: str | None = None,\n",
    "    index: int | str | None = None,\n",
    "    **kwargs: Any,\n",
    ") -> ToolCall:\n",
    "    \"\"\"Create a `ToolCall`.\n",
    "\n",
    "    Args:\n",
    "        name: The name of the tool to be called.\n",
    "        args: The arguments to the tool call.\n",
    "        id: An identifier for the tool call.\n",
    "\n",
    "            Generated automatically if not provided.\n",
    "        index: Index of block in aggregate response.\n",
    "\n",
    "            Used during streaming.\n",
    "\n",
    "    Returns:\n",
    "        A properly formatted `ToolCall`.\n",
    "\n",
    "    !!! note\n",
    "\n",
    "        The `id` is generated automatically if not provided, using a UUID4 format\n",
    "        prefixed with `'lc_'` to indicate it is a LangChain-generated ID.\n",
    "    \"\"\"\n",
    "    block = ToolCall(\n",
    "        type=\"tool_call\",\n",
    "        name=name,\n",
    "        args=args,\n",
    "        id=ensure_id(id),\n",
    "    )\n",
    "\n",
    "    if index is not None:\n",
    "        block[\"index\"] = index\n",
    "\n",
    "    extras = {k: v for k, v in kwargs.items() if v is not None}\n",
    "    if extras:\n",
    "        block[\"extras\"] = extras\n",
    "\n",
    "    return block\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19192c2",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef72291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-12 18:28:18.305808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1770920898.328380   14018 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1770920898.335390   14018 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1770920898.352646   14018 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770920898.352664   14018 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770920898.352667   14018 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1770920898.352669   14018 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Any, Dict, List, Optional, Union, Callable\n",
    "from abc import ABC, abstractmethod\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizer, PreTrainedModel,BitsAndBytesConfig\n",
    "import torch\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage,ToolMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from collections.abc import Callable, Mapping, Sequence\n",
    "from langchain_core.language_models import (\n",
    "    LanguageModelInput\n",
    ")\n",
    "from langchain_core.runnables import Runnable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "146a4050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerChatModel(BaseChatModel):\n",
    "\n",
    "  # for creating model\n",
    "  pretrained_model_name_or_path: str\n",
    "  device : str = \"auto\"\n",
    "  attn_implementation : str|None = None\n",
    "  quantization_config : Any = None\n",
    "  torch_dtype : Any = \"auto\"\n",
    "  hf_token : str|None = None # some model need authorized\n",
    "  # tools\n",
    "  bound_tools : List[Any] = []\n",
    "\n",
    "  # for generation\n",
    "  max_new_tokens: int = 512\n",
    "  temperature: float = 0.7\n",
    "  do_sample: bool = True\n",
    "  additional_generation_kwargs: dict[str, Any] = {}\n",
    "\n",
    "  # other\n",
    "  model: Any = None\n",
    "  tokenizer : Any = None\n",
    "  max_context_length : int = None\n",
    "\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      *,\n",
    "      model = None,\n",
    "      tokenizer = None,\n",
    "      **kwargs: Any\n",
    "  ):\n",
    "    # initial pydantic\n",
    "    super().__init__(**kwargs)\n",
    "\n",
    "    if model and tokenizer:\n",
    "      self.model = model\n",
    "      self.tokenizer = tokenizer\n",
    "    else:\n",
    "      torch_dtype = self.torch_dtype\n",
    "      if isinstance(torch_dtype, str):\n",
    "        if torch_dtype == \"float16\":\n",
    "          torch_dtype = torch.float16\n",
    "        elif torch_dtype == \"float32\":\n",
    "          torch_dtype = torch.float32\n",
    "        elif torch_dtype == \"bfloat16\":\n",
    "          torch_dtype = torch.bfloat16\n",
    "        elif torch_dtype == \"auto\":\n",
    "          torch_dtype = \"auto\"\n",
    "        else:\n",
    "          raise ValueError(f\"Invalid torch_dtype: {torch_dtype}\")\n",
    "      elif not isinstance(torch_dtype, torch.dtype):\n",
    "        raise ValueError(f\"Invalid torch_dtype: {torch_dtype}\")\n",
    "\n",
    "      load_model_config = {\n",
    "          \"pretrained_model_name_or_path\" : self.pretrained_model_name_or_path,\n",
    "          \"torch_dtype\" : torch_dtype,\n",
    "          \"device_map\" : self.device,\n",
    "          \"attn_implementation\" : self.attn_implementation,\n",
    "          \"quantization_config\" : self.quantization_config,\n",
    "          \"token\" : self.hf_token\n",
    "      }\n",
    "      print(\"Loading Model from hugginface...\\n\")\n",
    "      self.model = AutoModelForCausalLM.from_pretrained(**load_model_config)\n",
    "      self.tokenizer = AutoTokenizer.from_pretrained(self.pretrained_model_name_or_path)\n",
    "    # Set pad token if not exists\n",
    "    if self.tokenizer.pad_token is None:\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    # set max_context_length\n",
    "    if self.max_context_length is None:\n",
    "      try:\n",
    "        self.max_context_length = self.model.config.max_position_embeddings\n",
    "      except :\n",
    "        self.max_context_length = 1024\n",
    "\n",
    "  @property\n",
    "  def _generation_config(self) -> dict:\n",
    "    generation_config = {\n",
    "        \"max_new_tokens\": self.max_new_tokens,\n",
    "        \"temperature\": self.temperature,\n",
    "        \"do_sample\": self.do_sample,\n",
    "        \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "        **self.additional_generation_kwargs\n",
    "    }\n",
    "    return generation_config\n",
    "\n",
    "  def _generate(\n",
    "        self,\n",
    "        messages: list[BaseMessage],\n",
    "        stop: list[str] | None = None,\n",
    "        run_manager: CallbackManagerForLLMRun | None = None,\n",
    "        stream: bool | None = None,  # noqa: FBT001\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "    \n",
    "    tool_instruction = self.get_tools_desc()\n",
    "    if tool_instruction:\n",
    "      messages = [SystemMessage(content=tool_instruction)] + messages\n",
    "    # convert messages to prompt\n",
    "    hf_msg = convert_lc_message_to_hf_messages(messages)\n",
    "    # tokenize\n",
    "    inputs = self.tokenizer.apply_chat_template(\n",
    "        hf_msg,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=self.max_context_length,\n",
    "        return_dict = True\n",
    "    ).to(self.model.device)\n",
    "\n",
    "    generation_config = self._generation_config\n",
    "    with torch.inference_mode():\n",
    "      outputs = self.model.generate(\n",
    "        **inputs,\n",
    "        **generation_config\n",
    "      )\n",
    "    # Decode response\n",
    "    response = self.tokenizer.decode(\n",
    "      outputs[0][inputs['input_ids'].shape[1]:],\n",
    "      skip_special_tokens=True\n",
    "    ).strip()\n",
    "    \n",
    "    msg = AIMessage(content = response)\n",
    "    # post-process to extract tool_call here\n",
    "    # CODE\n",
    "    return ChatResult(generations=[ChatGeneration(message=msg)])\n",
    "\n",
    "\n",
    "\n",
    "  def get_tools_desc(self) -> str:\n",
    "    \"\"\"\n",
    "    Return all tools description in a string.\n",
    "    The tool must be converted into openai format.\n",
    "    Each description is converted into format:\n",
    "\n",
    "    name :\n",
    "        docstring.\n",
    "        params:\n",
    "          * param1 (type)\n",
    "          * param2 (type)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tools_desc = \"\"\n",
    "    for tool in self.bound_tools:\n",
    "      tool_info = tool[\"function\"]\n",
    "      tool_name = tool_info[\"name\"]\n",
    "      tool_desc = tool_info.get(\"description\", \"\")\n",
    "      if tool_desc:\n",
    "        tool_desc = \"\\n\\t\"+tool_desc\n",
    "      params_desc = []\n",
    "      for param_name, param_info in tool_info[\"parameters\"].get(\"properties\",{}).items():\n",
    "        param_type = param_info.get(\"type\", \"any\")\n",
    "        param_desc = param_info.get(\"description\", \"\")\n",
    "        additional_info = \";\".join([f\"{k}: {v}\" for k,v in param_info.items() if k != \"type\" and k != \"description\"])\n",
    "        if param_desc or additional_info:\n",
    "          param_desc = f\"\\t\\t{param_name} ({param_type}) -- {param_desc} {additional_info}\"\n",
    "        else:\n",
    "          param_desc = f\"\\t\\t{param_name} ({param_type})\"\n",
    "        params_desc.append(param_desc)\n",
    "\n",
    "      if not params_desc:\n",
    "        params_desc = \"This tool don't have parameters\"\n",
    "      else:\n",
    "        params_desc = \"\\n\".join(params_desc)\n",
    "\n",
    "      tool_desc = f\"\"\"**{tool_name}**:{tool_desc}\\n\\tparams:\\n{params_desc}\\n\\n\"\"\"\n",
    "      tools_desc += tool_desc\n",
    "\n",
    "    if tools_desc:\n",
    "      return f\"\"\"You have access to the following tools :\n",
    "\n",
    "{tools_desc}\n",
    "\n",
    "Whenever you need to use a tool, you MUST respond with a JSON code block in this exact format:\n",
    "```json\n",
    "{{\n",
    "    \"tool_calls\": [\n",
    "        {{\n",
    "            \"name\": \"tool_name\",\n",
    "            \"arguments\": {{\"param1\": \"value1\", \"param2\": \"value2\"}}\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "```\n",
    "Only use provided tools, do not invent new ones.\n",
    "If you don't need to use any tools, respond normally without the JSON block.\n",
    "Tool responses are provided with a tool_call_id.\n",
    "Always match each responese to the corresponding tool call using this ID and use the matched results to produce the final answer.\n",
    "A user query may contain multiple sub-questions.\n",
    "Use tools only for sub-questions that require them, answer the rest directly, and merge all results into the final response.\n",
    "Do not explain how you use tools.\n",
    "\"\"\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "  def bind_tools(\n",
    "      self,\n",
    "      tools: Sequence[dict[str, Any] | type | Callable | BaseTool],\n",
    "      *,\n",
    "      tool_choice: dict | str | bool | None = None,\n",
    "      **kwargs: Any,\n",
    "  ) -> Runnable[LanguageModelInput, AIMessage]:\n",
    "\n",
    "    # convert tools to open_ai format\n",
    "    formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n",
    "    # I add tool_choice so as model can work with agent\n",
    "    # (creating agent with tool will call model.bind_tool with too_choice)\n",
    "    # I want all my tools can be used\n",
    "    model_with_tools = self.__class__(\n",
    "        model = self.model,\n",
    "        tokenizer = self.tokenizer,\n",
    "        pretrained_model_name_or_path = self.pretrained_model_name_or_path,\n",
    "        device = self.device,\n",
    "        attn_implementation = self.attn_implementation,\n",
    "        quantization_config = self.quantization_config,\n",
    "        torch_dtype = self.torch_dtype,\n",
    "        hf_token = self.hf_token,\n",
    "        bound_tools = formatted_tools,\n",
    "        max_new_tokens = self.max_new_tokens,\n",
    "        temperature = self.temperature,\n",
    "        do_sample = self.do_sample,\n",
    "        **self.additional_generation_kwargs\n",
    "    )\n",
    "\n",
    "    return model_with_tools\n",
    "\n",
    "\n",
    "  @property\n",
    "  def _llm_type(self) -> str:\n",
    "    return \"transformer-chat-model\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b197cd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model from hugginface...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b784cf20ae4e48b960e47e51ffb1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_model_name_or_path = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "# Create your quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "chat_model = TransformerChatModel(\n",
    "    pretrained_model_name_or_path=pretrained_model_name_or_path,\n",
    "    quantization_config = bnb_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "416c4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def factorial(num: int) -> int:\n",
    "  \"\"\"Use to calculate the factorial of number or number!.\"\"\"\n",
    "  fact=1\n",
    "  for i in range(1,num+1):\n",
    "    fact=fact*i\n",
    "  return fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a578616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_tool = chat_model.bind_tools(tools = [factorial])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66be5000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n    \"tool_calls\": [\\n        {\\n            \"name\": \"factorial\",\\n            \"arguments\": {\"num\": \"5\"}\\n        }\\n    ]\\n}\\n```', additional_kwargs={}, response_metadata={}, id='run--054db17c-c5b4-4733-8a12-85db4bc55b91-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chat_with_tool.invoke(\"What is 5! ?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e642c670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_calls': [{'name': 'factorial', 'arguments': {'num': '5'}}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(response.content[8:-4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
